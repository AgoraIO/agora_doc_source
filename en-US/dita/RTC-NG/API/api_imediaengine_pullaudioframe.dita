<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="api_imediaengine_pullaudioframe">
    <title><ph keyref="pullAudioFrame"/></title>
    <shortdesc id="short"><ph id="shortdesc">Pulls the remote audio data.</ph></shortdesc>
    <prolog>
        <metadata>
            <keywords>
                <indexterm keyref="pullAudioFrame"/>
            </keywords>
        </metadata>
    </prolog>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public abstract int pullPlaybackAudioFrame(byte[] data, int lengthInByte);</codeblock>
            <codeblock props="hmos" outputclass="language-arkts"></codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">- (BOOL)pullPlaybackAudioFrameRawData:(void * _Nonnull)data
                              lengthInByte:(NSUInteger)lengthInByte;</codeblock>
            <codeblock props="cpp unreal" outputclass="language-cpp">virtual int pullAudioFrame(IAudioFrameObserver::AudioFrame* frame) = 0;</codeblock>
            <codeblock props="electron" outputclass="language-typescript">abstract pullAudioFrame(): AudioFrame;</codeblock>
            <codeblock props="unity cs" outputclass="language-csharp">public abstract int PullAudioFrame(AudioFrame frame);</codeblock>
            <codeblock props="rn" outputclass="language-typescript">abstract pullAudioFrame(): AudioFrame;</codeblock>
            <codeblock props="flutter" outputclass="language-dart">Future&lt;void&gt; pullAudioFrame(AudioFrame frame);</codeblock>
            <codeblock props="reserve" outputclass="language-cpp"></codeblock></p>
        </section>
        <section id="detailed_desc" deliveryTarget="details" otherprops="no-title">
            <p>After a successful call of this method, the app pulls the decoded and mixed audio data for playback.</p>
        </section>
        <section id="timing" deliveryTarget="details">
            <title>Call timing</title>
             <p>Call this method after joining a channel.</p>
             <p>Before calling this method, call <xref keyref="setExternalAudioSink"/><codeph>(<parmname>enabled</parmname>: <ph keyref="true"/>) to</codeph> notify the app to enable and set the external audio rendering.</p>
        </section>        
        <section id="restriction" deliveryTarget="details">
            <title>Restrictions</title>
            <p>Both this method and<xref keyref="onPlaybackAudioFrame"/>  callback can be used to get audio data after remote mixing. Once you<apiname keyref="setExternalAudioSink"/> enable the external audio sink, the app will not<apiname keyref="onPlaybackAudioFrame"/> retrieve any audio data from the  callback. 因此，请根据实际的业务需求在该方法和 <apiname keyref="onPlaybackAudioFrame"/> 回调之间进行选择。 The specific distinctions between them are as follows:<ul>
                <li>After calling this method, the app automatically pulls the audio data from the SDK. By setting the audio data parameters, the SDK adjusts the frame buffer to help the app handle latency, effectively avoiding audio playback jitter.</li>
                <li>注册 <apiname keyref="onPlaybackAudioFrame"/> 回调后，SDK 通过该回调将音频数据传输给 App。 App 在处理音频帧的延迟时，可能会导致音频播放抖动。</li>
                </ul></p>
            <p>该方法仅用于拉取远端混音后的音频播放数据，如需获取采集的原始音频数据、混音前每条拉流单独的原始音频播放数据等，可以通过调用 <xref keyref="registerAudioFrameObserver"/> 注册相应的回调。</p>
        </section>
        <section id="parameters" deliveryTarget="details">
            <title><ph props="android apple cpp unreal bp flutter unity cs">Parameters</ph></title>
            <parml props="android apple cpp unreal bp flutter unity cs">
            <plentry props="cpp unreal bp unity flutter cs">
                <pt>frame</pt>
                <pd>Pointers to <xref keyref="AudioFrame"/>.</pd>
            </plentry>
            <plentry props="android ios mac">
                <pt>data</pt>
                <pd>The remote audio data to be pulled. The data type is <codeph>byte[]</codeph>.</pd>
            </plentry>
            <plentry props="android ios mac" id="length">
                <pt>lengthInByte</pt>
                <pd>The data length (byte). The value of this parameter is related to the audio duration, and the values of the <codeph>sampleRate</codeph> and <codeph>channels</codeph> parameters that you set in <apiname keyref="setExternalAudioSink"/>. <codeph>lengthInByte</codeph> = <codeph>sampleRate</codeph>/1000 × 2 × <codeph>channels</codeph> × audio duration (ms).</pd>
            </plentry>
            </parml> </section>
        <section id="return_values">
            <title><ph keyref="return-section-title"/></title>
            <p props="flutter">When the method call succeeds, there is no return value; when fails, the <xref keyref="AgoraRtcException"/> exception is thrown. You need to catch the exception and handle it accordingly. <ph props="cn">See <xref keyref="error-code-link"/> for details and resolution suggestions.</ph></p>
            <ul props="android cpp unreal bp unity cs">
            <li>0: Success.</li>
            <li>&lt; 0: Failure. <ph props="cn">See <xref keyref="error-code-link"/> for details and resolution suggestions.</ph></li>
            </ul>
            <ul props="ios mac">
            <li><codeph><ph keyref="true"/></codeph>: Success.</li>
            <li><codeph><ph keyref="false"/></codeph>: Failure. <ph props="cn">See <xref keyref="error-code-link"/> for details and resolution suggestions.</ph></li>
            </ul>
            <ul props="electron rn">
            <li>The <apiname keyref="AudioFrame" /> instance, if the method call succeeds.</li>
            <li>An error code, if the call fails,.</li>
            </ul> </section>
        </refbody>
</reference>
