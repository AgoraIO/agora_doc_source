<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="api_irtcengine_setaudioscenario">
    <title><ph keyref="setAudioScenario" /></title>
    <shortdesc id="short"><ph id="shortdesc">Sets audio scenarios.</ph></shortdesc>
    <prolog>
        <metadata>
            <keywords>
                <indexterm keyref="setAudioScenario" />
            </keywords>
        </metadata>
    </prolog>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public abstract int setAudioScenario(int scenario);</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">- (int)setAudioScenario:(AgoraAudioScenario)scenario;</codeblock>
            <codeblock props="cpp" outputclass="language-cpp">virtual int setAudioScenario(AUDIO_SCENARIO_TYPE scenario) = 0;</codeblock>
            <codeblock props="cs" outputclass="language-csharp" />
            <codeblock props="electron" outputclass="language-typescript">abstract setAudioScenario(scenario: AudioScenarioType): number;</codeblock>
            <codeblock props="unity" outputclass="language-csharp">public abstract int SetAudioScenario(AUDIO_SCENARIO_TYPE scenario);</codeblock>
            <codeblock props="rn" outputclass="language-typescript">abstract setAudioScenario(scenario: AudioScenarioType): number;</codeblock>
            <codeblock props="flutter" outputclass="language-dart">Future&lt;void&gt; setAudioScenario(AudioScenarioType scenario);</codeblock>
            <codeblock props="unreal" outputclass="language-cpp" /></p>
        </section>
        <section id="detailed_desc">
            <note>You can call this method either before or after joining a channel.</note> </section>
        <section id="parameters">
            <title>Parameters</title>
            <parml>
            <plentry>
                <pt>scenario</pt>
                <pd props="cpp mac ios framework">The audio scenarios. See <xref keyref="AUDIO_SCENARIO_TYPE" />. Under different audio scenarios, the device uses different volume types.</pd>
                <pd props="android">The audio scenarios:<ul><li><ph keyref="AUDIO_SCENARIO_DEFAULT" />(0): (Default) Automatic scenario, where the SDK chooses the appropriate audio quality according to the user role and audio route.</li>
                    <li><ph keyref="AUDIO_SCENARIO_GAME_STREAMING" />(3): High-quality audio scenario, where users mainly play music.</li>
                    <li><ph keyref="AUDIO_SCENARIO_CHATROOM" />(5): Chatroom scenario, where users need to frequently switch the user role or mute and unmute the microphone. In this scenario, audience members receive a pop-up window to request permission of using microphones.</li>
                    <li><ph keyref="AUDIO_SCENARIO_CHORUS" />(7): Real-time chorus scenario,  where users have good network conditions and require extremely low latency.<note>Before using this enumeration, you need to call <xref keyref="getAudioDeviceInfo" /> to see whether the audio device supports ultra-low-latency capture and playback. To experience ultra-low latency, you need to ensure that your audio device supports ultra-low latency (<parmname>isLowLatencyAudioSupported</parmname> = <codeph><ph keyref="true" /></codeph>).</note></li>
                    <li><ph keyref="AUDIO_SCENARIO_MEETING" />(8): Meeting scenario that mainly involves the human voice.</li></ul>
                </pd>
            </plentry>
            </parml> </section>
        <section id="return_values" props="native rn electron unity">
            <title>Returns</title>
            <ul>
            <li>0: Success.</li>
            <li>&lt; 0: Failure.</li>
            </ul> </section>
    </refbody>
</reference>