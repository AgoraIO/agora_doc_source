<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="class_audioframe">
    <title><ph keyref="AudioFrame" /></title>
    <shortdesc id="short"><ph id="shortdesc">Raw audio data.</ph></shortdesc>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public class AudioFrame {
  public ByteBuffer buffer;
  public int sampleRataHz;
  public int bytesPerSample;
  public int channelNums;
  public int samplesPerChannel;
  public long timestamp;

  @CalledByNative
  public AudioFrame(ByteBuffer buffer, int sampleRataHz, int bytesPerSample, int channelNums,
      int samplesPerChannel, long timestamp) {
    this.sampleRataHz = sampleRataHz;
    this.bytesPerSample = bytesPerSample;
    this.channelNums = channelNums;
    this.samplesPerChannel = samplesPerChannel;
    this.timestamp = timestamp;
    this.buffer = buffer;
  }

  @CalledByNative
  public ByteBuffer getByteBuffer() {
    return buffer;
  }

  @CalledByNative
  public int getBytesPerSample() {
    return bytesPerSample;
  }

  @CalledByNative
  public int getChannelNums() {
    return channelNums;
  }

  @CalledByNative
  public int getSampleRataHz() {
    return sampleRataHz;
  }

  @CalledByNative
  public int getSamplesPerChannel() {
    return samplesPerChannel;
  }

  @CalledByNative
  public long getTimestamp() {
    return timestamp;
  }

  @Override
  public String toString() {
    return "AudioFrame{sampleRataHz=" + sampleRataHz + ", bytesPerSample=" + bytesPerSample
        + ", channelNums=" + channelNums + ", samplesPerChannel=" + samplesPerChannel
        + ", timestamp=" + timestamp + '}';
  }
}</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">
__attribute__((visibility("default"))) @interface AgoraAudioFrame : NSObject

@property(assign, nonatomic) NSInteger samplesPerChannel;

@property(assign, nonatomic) NSInteger bytesPerSample;

@property(assign, nonatomic) NSInteger channels;

@property(assign, nonatomic) NSInteger samplesPerSec;

@property(assign, nonatomic) void* _Nullable buffer;

@property(assign, nonatomic) int64_t renderTimeMs;

@property(assign, nonatomic) NSInteger avSyncType;
@end</codeblock>
            <codeblock props="cpp unreal" outputclass="language-cpp">  struct AudioFrame {
    AUDIO_FRAME_TYPE type;
    int samplesPerChannel;
    agora::rtc::BYTES_PER_SAMPLE bytesPerSample;
    int channels;
    int samplesPerSec;
    void* buffer;
    int64_t renderTimeMs;
    int avsync_type;

    AudioFrame() : type(FRAME_TYPE_PCM16),
                   samplesPerChannel(0),
                   bytesPerSample(rtc::TWO_BYTES_PER_SAMPLE),
                   channels(0),
                   samplesPerSec(0),
                   buffer(NULL),
                   renderTimeMs(0),
                   avsync_type(0),
  };
</codeblock>
         <codeblock props="bp" outputclass="language-cpp">USTRUCT(BlueprintType)
struct FAudioFrame {

	GENERATED_BODY()
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	EAUDIO_FRAME_TYPE type;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int samplesPerChannel;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	EBYTES_PER_SAMPLE bytesPerSample;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int channels;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int samplesPerSec;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int64 buffer;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int64 renderTimeMs;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int avsync_type;
};</codeblock>
            <codeblock props="electron" outputclass="language-typescript">export class AudioFrame {
  type?: AudioFrameType;
  samplesPerChannel?: number;
  bytesPerSample?: BytesPerSample;
  channels?: number;
  samplesPerSec?: number;
  buffer?: Uint8Array;
  renderTimeMs?: number;
  avsync_type?: number;
}</codeblock>
            <codeblock props="unity cs" outputclass="language-csharp">public class AudioFrame
    {
        public AudioFrame()
        {
            type = AUDIO_FRAME_TYPE.FRAME_TYPE_PCM16;
            samplesPerChannel = 0;
            bytesPerSample = BYTES_PER_SAMPLE.TWO_BYTES_PER_SAMPLE;
            channels = 0;
            samplesPerSec = 0;
            RawBuffer = new byte[0];
            renderTimeMs = 0;
            avsync_type = 0;
        }

        public AudioFrame(AUDIO_FRAME_TYPE type, int samplesPerChannel, BYTES_PER_SAMPLE bytesPerSample, int channels, int samplesPerSec,
            byte[] buffer, long renderTimeMs, int avsync_type)
        {
            this.type = type;
            this.samplesPerChannel = samplesPerChannel;
            this.bytesPerSample = bytesPerSample;
            this.channels = channels;
            this.samplesPerSec = samplesPerSec;
            this.RawBuffer = buffer;
            this.renderTimeMs = renderTimeMs;
            this.avsync_type = avsync_type;
        }

        public AUDIO_FRAME_TYPE type { set; get; }
        public int samplesPerChannel { set; get; }
        public BYTES_PER_SAMPLE bytesPerSample { set; get; }
        public int channels { set; get; }
        public int samplesPerSec { set; get; }
        public byte[] RawBuffer { set; get; }
        public long renderTimeMs { set; get; }
        public int avsync_type { set; get; }
    }</codeblock>
            <codeblock props="rn" outputclass="language-typescript">export class AudioFrame {
  type?: AudioFrameType;
  samplesPerChannel?: number;
  bytesPerSample?: BytesPerSample;
  channels?: number;
  samplesPerSec?: number;
  buffer?: Uint8Array;
  renderTimeMs?: number;
  avsync_type?: number;
}</codeblock>
            <codeblock props="flutter" outputclass="language-dart">class AudioFrame {
  const AudioFrame(
      {this.type,
      this.samplesPerChannel,
      this.bytesPerSample,
      this.channels,
      this.samplesPerSec,
      this.buffer,
      this.renderTimeMs,
      this.avsyncType});</codeblock>
            <codeblock props="reserve" outputclass="language-cpp"></codeblock></p>
        </section>
        <section id="parameters">
            <title> <text conref="../conref/conref_api_metadata.dita#conref_api_metadata/property" /> </title>
            <parml>
            <plentry props="cpp unreal bp unity rn electron flutter cs">
                <pt>type</pt>
                <pd>
                    <p>The type of the audio frame. See <xref keyref="AUDIO_FRAME_TYPE" />.</p>
                </pd>
            </plentry>
            <plentry>
                <pt>samplesPerChannel</pt>
                <pd>The number of samples per channel in the audio frame.</pd>
            </plentry>
            <plentry>
                <pt>bytesPerSample</pt>
                <pd>The number of bytes per sample. For PCM, this parameter is generally set to 16 bits (2 bytes).</pd>
            </plentry>
            <plentry>
                <pt><ph props="cpp unreal bp ios mac unity electron rn flutter cs">channels</ph><ph props="android">channelNums</ph></pt>
                <pd>
                    <p>The number of audio channels (the data are interleaved if it is stereo).<ul id="ul_zxz_2wt_r4b">
                    <li>1: Mono.</li>
                    <li>2: Stereo.</li>
                    </ul> </p>
                </pd>
            </plentry>
            <plentry>
                <pt props="cpp unreal bp ios mac unity electron rn flutter cs">samplesPerSec</pt>
                <pt props="android">sampleRataHz</pt>
                <pd>The number of samples per channel in the audio frame.</pd>
            </plentry>
            <plentry>
                <pt props="android cpp unreal bp ios mac electron rn flutter">buffer</pt>
                <pt props="unity cs">RawBuffer</pt>
                <pd>
                    <p>The data buffer of the audio frame. When the audio frame uses a stereo channel, the data buffer is interleaved.</p>
                    <p>The size of the data buffer is as follows: <codeph>buffer</codeph> = <codeph>samples</codeph> × <codeph>channels</codeph> × <codeph>bytesPerSample</codeph>.</p>
                </pd>
            </plentry>
            <plentry props="cpp unreal bp unity ios mac rn electron flutter cs">
                <pt>renderTimeMs</pt>
                <pd>
                    <p>The timestamp (ms) of the external audio frame.</p>
                    <p>You can use this timestamp to restore the order of the captured audio frame, and synchronize audio and video frames in video scenarios, including scenarios where external video sources are used.</p>
                </pd>
            </plentry>
            <plentry props="ios mac cpp unreal bp unity flutter electron rn cs">
                <pt><ph props="ios mac flutter">avsyncType</ph><ph props="cpp unreal bp unity electron rn cs">avsync_type</ph></pt>
                <pd>Reserved for future use.</pd>
            </plentry>
            <plentry props="android">
                <pt>timestamp</pt>
                <pd>The timestamp (ms) of the audio frame.</pd>
            </plentry>
            </parml> </section>
    </refbody>
</reference>