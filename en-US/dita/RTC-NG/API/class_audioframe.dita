<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="class_audioframe">
    <title><ph keyref="AudioFrame" /></title>
    <shortdesc id="short"><ph id="shortdesc">Raw audio data.</ph></shortdesc>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public class AudioFrame {
    public byte[] bytes;
    public int sampleRataHz;
    public int bytesPerSample;
    public int channelNums;
    public int samplesPerChannel;
    public long timestamp;
    @CalledByNative
    public AudioFrame(byte[] bytes, int sampleRataHz, int bytesPerSample, int channelNums,
        int samplesPerChannel, long timestamp) {
      this.sampleRataHz = sampleRataHz;
      this.bytesPerSample = bytesPerSample;
      this.channelNums = channelNums;
      this.samplesPerChannel = samplesPerChannel;
      this.timestamp = timestamp;
      this.bytes = bytes;
    }
    @CalledByNative
    public byte[] getBytes() {
      return bytes;
    }
    @CalledByNative
    public int getBytesPerSample() {
      return bytesPerSample;
    }
    @CalledByNative
    public int getChannelNums() {
      return channelNums;
    }
    @CalledByNative
    public int getSampleRataHz() {
      return sampleRataHz;
    }
    @CalledByNative
    public int getSamplesPerChannel() {
      return samplesPerChannel;
    }
    @CalledByNative
    public long getTimestamp() {
      return timestamp;
    }
    @Override
    public String toString() {
      return "AudioFrame{sampleRataHz=" + sampleRataHz + ", bytesPerSample=" + bytesPerSample
          + ", channelNums=" + channelNums + ", samplesPerChannel=" + samplesPerChannel
          + ", timestamp=" + timestamp + '}';
    }
  }
</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">__attribute__((visibility("default"))) @interface AgoraAudioFrame: NSObject
 @property (assign, nonatomic) NSInteger samplesPerChannel;
 @property (assign, nonatomic) NSInteger bytesPerSample;
 @property (assign, nonatomic) NSInteger channels;
 @property (assign, nonatomic) NSInteger samplesPerSec;
 @property (strong, nonatomic) NSData* _Nullable buffer;
 @property (assign, nonatomic) int64_t renderTimeMs;
 @property (assign, nonatomic) NSInteger avSyncType;
@end</codeblock>
            <codeblock props="cpp" outputclass="language-cpp">struct AudioFrame {
    AUDIO_FRAME_TYPE type;
    int samplesPerChannel;
    agora::rtc::BYTES_PER_SAMPLE bytesPerSample;
    int channels;
    int samplesPerSec;
    void* buffer;
    int64_t renderTimeMs;
    int avsync_type;
    AudioFrame() : type(FRAME_TYPE_PCM16),
                   samplesPerChannel(0),
                   bytesPerSample(rtc::TWO_BYTES_PER_SAMPLE),
                   channels(0),
                   samplesPerSec(0),
                   buffer(NULL),
                   renderTimeMs(0),
                   avsync_type(0) {}
  };
</codeblock>
            <codeblock props="electron" outputclass="language-typescript">export class AudioFrame {
  
  type?: AudioFrameType;
  
  samplesPerChannel?: number;
  
  bytesPerSample?: BytesPerSample;
  
  channels?: number;
  
  samplesPerSec?: number;
  
  buffer?: Uint8Array;
  
  renderTimeMs?: number;
  
  avsync_type?: number;
}</codeblock>
            <codeblock props="unity" outputclass="language-csharp">public class AudioFrame
    {
        public AudioFrame()
        {
            type = AUDIO_FRAME_TYPE.FRAME_TYPE_PCM16;
            samplesPerChannel = 0;
            bytesPerSample = BYTES_PER_SAMPLE.TWO_BYTES_PER_SAMPLE;
            channels = 0;
            samplesPerSec = 0;
            RawBuffer = new byte[0];
            renderTimeMs = 0;
            avsync_type = 0;
        }

        public AudioFrame(AUDIO_FRAME_TYPE type, int samplesPerChannel, BYTES_PER_SAMPLE bytesPerSample, int channels, int samplesPerSec,
            byte[] buffer, long renderTimeMs, int avsync_type)
        {
            this.type = type;
            this.samplesPerChannel = samplesPerChannel;
            this.bytesPerSample = bytesPerSample;
            this.channels = channels;
            this.samplesPerSec = samplesPerSec;
            this.RawBuffer = buffer;
            this.renderTimeMs = renderTimeMs;
            this.avsync_type = avsync_type;
        }

        public AUDIO_FRAME_TYPE type { set; get; }
        public int samplesPerChannel { set; get; }
        public BYTES_PER_SAMPLE bytesPerSample { set; get; }
        public int channels { set; get; }
        public int samplesPerSec { set; get; }
        public byte[] RawBuffer { set; get; }
        public long renderTimeMs { set; get; }
        public int avsync_type { set; get; }
    }</codeblock>
            <codeblock props="rn" outputclass="language-typescript">export class AudioFrame {
  
  type?: AudioFrameType;
  
  samplesPerChannel?: number;
  
  bytesPerSample?: BytesPerSample;
  
  channels?: number;
  
  samplesPerSec?: number;
  
  buffer?: Uint8Array;
  
  renderTimeMs?: number;
  
  avsync_type?: number;
}</codeblock>
            <codeblock props="flutter" outputclass="language-dart">There are no corresponding names available</codeblock> </p>
        </section>
        <section id="parameters">
            <title> <text conref="../conref/conref_api_metadata.dita#conref_api_metadata/property" /> </title>
            <parml>
            <plentry props="cpp unity">
                <pt>type</pt>
                <pd>
                    <p>The type of the audio frame. See <xref keyref="AUDIO_FRAME_TYPE" />.</p>
                </pd>
            </plentry>
            <plentry>
                <pt>samplesPerChannel</pt>
                <pd>The number of samples per channel in the audio frame.</pd>
            </plentry>
            <plentry>
                <pt>bytesPerSample</pt>
                <pd>The number of bytes per audio sample, which is usually 16-bit (2 bytes).</pd>
            </plentry>
            <plentry>
                <pt><ph props="cpp ios mac unity">channels</ph><ph props="android">channelNums</ph></pt>
                <pd>
                    <p>The number of audio channels (the data are interleaved if it is stereo).
                    <ul id="ul_zxz_2wt_r4b">
                    <li>1: Mono.</li>
                    <li>2: Stereo.</li>
                    </ul> </p>
                </pd>
            </plentry>
            <plentry>
                <pt props="cpp ios mac unity">samplesPerSec</pt>
                <pt props="android">sampleRataHz</pt>
                <pd>The number of samples per channel in the audio frame.</pd>
            </plentry>
            <plentry>
                <pt props="cpp ios mac">buffer</pt>
                    <pt props="android">bytes</pt>
                <pt props="unity">RawBuffer</pt>
                <pd>
                    <p>The data buffer of the audio frame. When the audio frame uses a stereo channel, the data buffer is interleaved.</p>
                    <p>The size of the data buffer is as follows: <codeph>buffer</codeph> = <codeph>samples</codeph> × <codeph>channels</codeph> × <codeph>bytesPerSample</codeph>.</p>
                </pd>
            </plentry>
            <plentry props="cpp unity ios mac">
                <pt>renderTimeMs</pt>
                <pd>
                    <p>The timestamp (ms) of the external audio frame.</p>
                    <p>You can use this timestamp to restore the order of the captured audio frame, and synchronize audio and video frames in video scenarios, including scenarios where external video sources are used.</p>
                </pd>
            </plentry>
            <plentry props="ios mac cpp unity">
                <pt><ph props="ios mac">avsyncType</ph> <ph props="cpp unity">avsync_type</ph> </pt>
                <pd>Reserved for future use.</pd>
            </plentry>
            <plentry props="android">
                <pt>timestamp</pt>
                <pd>The timestamp (ms) of the audio frame.</pd>
            </plentry>
            </parml> </section>
    </refbody>
</reference>