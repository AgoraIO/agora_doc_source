<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="callback_iaudioframeobserverbase_getobservedaudioframeposition">
    <title><ph keyref="getObservedAudioFramePosition"/></title>
    <shortdesc id="short"><ph id="shortdesc">Sets the frame position for the video observer.</ph></shortdesc>
    <prolog>
        <metadata>
            <keywords>
                <indexterm keyref="getObservedAudioFramePosition"/>
            </keywords>
        </metadata>
    </prolog>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">- (AgoraAudioFramePosition)getObservedAudioFramePosition NS_SWIFT_NAME(getObservedAudioFramePosition());</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">- (AgoraAudioFramePosition)getObservedAudioFramePosition NS_SWIFT_NAME(getObservedAudioFramePosition());</codeblock>
            <codeblock props="cpp" outputclass="language-cpp">virtual int getObservedAudioFramePosition() = 0;</codeblock>
            <codeblock props="cs" outputclass="language-csharp"/>
            <codeblock props="electron" outputclass="language-typescript"/>
            <codeblock props="unity" outputclass="language-csharp">public virtual int GetObservedAudioFramePosition()</codeblock>
            <codeblock props="rn" outputclass="language-typescript"/>
           <codeblock props="flutter" outputclass="language-dart"/> 
            <codeblock props="unreal" outputclass="language-cpp"></codeblock></p>
        </section>
        <section id="detailed_desc">
            <p>After successfully registering the audio data observer, the SDK uses this callback for each specific audio frame processing node to determine whether to trigger the following callbacks:<ul>
                <li><xref keyref="onRecordAudioFrame"/></li>
                <li><xref keyref="onPlaybackAudioFrame"/></li>
                <li><xref keyref="onPlaybackAudioFrameBeforeMixing"/></li>
                <li><xref keyref="onMixedAudioFrame"/></li>
                <li><xref keyref="onEarMonitoringAudioFrame"/></li>
            </ul></p>
            <p>You can set one or more positions you need to observe by modifying the return value of <apiname keyref="getObservedAudioFramePosition"/> based on your scenario requirements:</p>
            <p>When the annotation observes multiple locations, the | (or operator) is required. To conserve system resources, you can reduce the number of frame positions that you want to observe.</p>
        </section>
        <section id="return_values">
            <title>Returns</title>   
            <p>Returns a bitmask that sets the observation position, with the following values:<ul>
                <li><ph keyref="AUDIO_FRAME_POSITION_PLAYBACK"/>(0x0001): This position can observe the playback audio mixed by all remote users, corresponding to the <apiname keyref="onPlaybackAudioFrame"/> callback.</li>
                <li><ph keyref="AUDIO_FRAME_POSITION_RECORD"/>(0x0002): This position can observe the collected local user's audio, corresponding to the <apiname keyref="onRecordAudioFrame"/> callback.</li>
                <li><ph keyref="AUDIO_FRAME_POSITION_MIXED"/>(0x0004): This position can observe the playback audio mixed by the loacl user and all remote users, corresponding to the <apiname keyref="onMixedAudioFrame"/> callback.</li>
                <li><ph keyref="AUDIO_FRAME_POSITION_BEFORE_MIXING"/>(0x0008): This position can observe the audio of a single remote user before mixing, corresponding to the <apiname keyref="onPlaybackAudioFrameBeforeMixing"/> callback.</li>
                <li><ph keyref="AUDIO_FRAME_POSITION_EAR_MONITORING"/>(0x0010): This position can observe the audio of a single remote user before mixing, corresponding to the <apiname keyref="onEarMonitoringAudioFrame"/> callback.</li>
            </ul></p>
        </section>
    </refbody>
</reference>
