<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="callback_iaudioframeobserverbase_onearmonitoringaudioframe">
    <title><ph keyref="onEarMonitoringAudioFrame"/></title>
    <shortdesc id="short"><ph id="shortdesc">Gets the in-ear monitoring audio frame.</ph></shortdesc>
    <prolog>
        <metadata>
            <keywords>
                <indexterm keyref="onEarMonitoringAudioFrame"/>
            </keywords>
        </metadata>
    </prolog>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public abstract boolean onEarMonitoringAudioFrame(int type, int samplesPerChannel,
    int bytesPerSample, int channels, int samplesPerSec, ByteBuffer buffer, long renderTimeMs,
    int avsync_type);
</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">- (BOOL)onEarMonitoringAudioFrame:(AgoraAudioFrame* _Nonnull)frame;
</codeblock>
            <codeblock props="cpp" outputclass="language-cpp">virtual bool onEarMonitoringAudioFrame(AudioFrame&amp; audioFrame) = 0;</codeblock>
            <codeblock props="cs" outputclass="language-csharp"/>
            <codeblock props="electron" outputclass="language-typescript">onEarMonitoringAudioFrame?(audioFrame: AudioFrame): boolean;</codeblock>
            <codeblock props="unity" outputclass="language-csharp">public virtual bool OnEarMonitoringAudioFrame(AudioFrame audioFrame)
        {
            return true;
        }</codeblock>
            <codeblock props="rn" outputclass="language-typescript">onEarMonitoringAudioFrame?(audioFrame: AudioFrame): boolean;</codeblock>
            <codeblock props="flutter" outputclass="language-dart">final void Function(AudioFrame audioFrame)? onEarMonitoringAudioFrame;</codeblock> </p>
        </section>
        <section id="detailed_desc">
            <p>为保证耳返的音频数据格式符合预期，你可以在如下两种方法中任选一种，设置耳返音频数据格式：                           
            
            <ul>
            <li>Method 1: After calling <xref keyref="setEarMonitoringAudioFrameParameters"/> to set the audio data format, call <xref keyref="registerAudioFrameObserver"/> to register the audio observer object, the SDK calculates the sampling interval according to the parameters in this method, and triggers the <apiname keyref="onEarMonitoringAudioFrame"/> callback according to the sampling interval.</li>
            <li>Method 2: After calling <xref keyref="registerAudioFrameObserver"/> to register audio observer object, set the specific audio observation position in the return value of the <xref keyref="getObservedAudioFramePosition"/> callback, and then set the audio data format in the return value of the <xref keyref="getEarMonitoringAudioParams"/> callback. The SDK calculates the sampling interval according to the return value of the callback, and triggers the <apiname keyref="onEarMonitoringAudioFrame"/> callback according to the sampling interval.</li>
            </ul> </p>
            <note type="note" id="attention">方法一的优先级高于方法二，如果已使用方法一设置了音频数据格式，则使用方法二的设置不生效。</note></section>
        <section id="parameters" conkeyref="onMixedAudioFrame/parameters"/>
        <section id="return_values" conkeyref="onMixedAudioFrame/return_values"/>
    </refbody>
</reference>
