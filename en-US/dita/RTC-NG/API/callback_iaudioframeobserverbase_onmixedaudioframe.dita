<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="callback_iaudioframeobserverbase_onmixedaudioframe">
    <title><ph keyref="onMixedAudioFrame"/></title>
    <shortdesc id="short"><ph id="shortdesc">Retrieves the mixed captured and playback audio frame.</ph></shortdesc>
    <prolog>
        <metadata>
            <keywords>
                <indexterm keyref="onMixedAudioFrame"/>
            </keywords>
        </metadata>
    </prolog>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public abstract boolean onMixedAudioFrame(int type, int samplesPerChannel, int bytesPerSample,
      int channels, int samplesPerSec, ByteBuffer buffer, long renderTimeMs, int avsync_type);
</codeblock>
            <codeblock props="hmos" outputclass="language-arkts">onMixedAudioFrame?: (channelId: string, type: number, samplesPerChannel: number, bytesPerSample: number, channels: number, samplesPerSec: number, buffer: ArrayBuffer, renderTimeMs: number, avsync_type: number) =&gt; boolean;</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">- (BOOL)onMixedAudioFrame:(AgoraAudioFrame* _Nonnull)frame channelId:(NSString * _Nonnull)channelId  NS_SWIFT_NAME(onMixedAudioFrame(_:channelId:));</codeblock>
            <codeblock props="cpp unreal" outputclass="language-cpp">virtual bool onMixedAudioFrame(const char* channelId, AudioFrame&amp; audioFrame) = 0;</codeblock>
         <codeblock props="bp" outputclass="language-cpp">DECLARE_DYNAMIC_MULTICAST_DELEGATE_TwoParams(FOnMixedAudioFrame, const FString, channelId, const FAudioFrame&amp;, audioFrame);</codeblock>
            <codeblock props="electron" outputclass="language-typescript">  onMixedAudioFrame?(channelId: string, audioFrame: AudioFrame): void;</codeblock>
            <codeblock props="unity cs" outputclass="language-csharp">public virtual bool OnMixedAudioFrame(string channelId, AudioFrame audio_frame)
        {
            return true;
        }</codeblock>
            <codeblock props="rn" outputclass="language-typescript">  onMixedAudioFrame?(channelId: string, audioFrame: AudioFrame): void;</codeblock>
            <codeblock props="flutter" outputclass="language-dart">final void Function(String channelId, AudioFrame audioFrame)?
      onMixedAudioFrame;</codeblock>
            <codeblock props="reserve" outputclass="language-cpp"/></p>
        </section>
        <section id="detailed_desc">
            <p props="native unreal bp cs">To ensure that the data format of mixed captured and playback audio frame meets the expectations, Agora recommends that you choose one of the following two ways to set the data format:<ul>
            <li>Method 1: After calling <xref keyref="setMixedAudioFrameParameters"/> to set the audio data format and <xref keyref="registerAudioFrameObserver"/> to register the audio frame observer object, the SDK calculates the sampling interval according to the parameters set in the methods, and triggers the <apiname keyref="onMixedAudioFrame"/> callback according to the sampling interval.</li>
            <li>Method 2: After calling <xref keyref="registerAudioFrameObserver"/> to register the audio frame observer object, set the audio data format in the return value of the <xref keyref="getObservedAudioFramePosition"/> callback. The SDK then calculates the sampling interval according to the return value of the <xref keyref="getMixedAudioParams"/> callback, and triggers the <apiname keyref="onMixedAudioFrame"/> callback according to the sampling interval.</li>
            </ul> </p>
            <p props="flutter rn electron">To ensure that the data format of mixed captured and playback audio frame meets the expectations, Agora recommends that you set the data format as follows: After calling <xref keyref="setMixedAudioFrameParameters"/> to set the audio data format and <xref keyref="registerAudioFrameObserver"/> to register the audio frame observer object, the SDK calculates the sampling interval according to the parameters set in the methods, and triggers the <apiname keyref="onMixedAudioFrame"/> callback according to the sampling interval.</p>
            <note type="note" id="attention" props="native unreal bp flutter cs">
            <ul>
            <li props="native unreal bp cs">The priority of method 1 is higher than that of method 2. If method 1 is used to set the audio data format, the setting of method 2 is invalid.</li>
            <li props="flutter rn electron">Due to framework limitations, this callback does not support sending processed audio data back to the SDK.</li>
            </ul></note>
        <p props="unity">To ensure that the data format of mixed captured and playback audio frame meets the expectations, Agora recommends that you set the data format as follows: After calling <xref keyref="setMixedAudioFrameParameters"/> to set the audio data format and <xref keyref="registerAudioFrameObserver"/> to register the audio frame observer object, the SDK calculates the sampling interval according to the parameters set in the methods, and triggers the <apiname keyref="onMixedAudioFrame"/> callback according to the sampling interval.</p></section>
        <section id="parameters">
            <title>Parameters</title>
            <parml>
            <plentry>
                <pt props="cpp unreal bp electron rn flutter">audioFrame</pt>
                <pt props="unity cs">audio_Frame</pt>
                <pt props="ios mac">frame</pt>
                <pd>The raw audio data. See <xref keyref="AudioFrame"/>. </pd>
            </plentry>
            <plentry props="ios mac cpp unreal bp unity rn electron flutter cs" id="id">
                <pt>channelId</pt>
                <pd>The channel ID.</pd>
            </plentry>
            </parml>
            <parml props="android hmos">
            <plentry id="type">
                <pt>type</pt>
                <pd>The audio frame type.</pd>
            </plentry>
            <plentry id="samplesPerChannel">
                <pt>samplesPerChannel</pt>
                <pd>The number of samples per channel in the audio frame.</pd>
            </plentry>
            <plentry id="bytesPerSample">
                <pt>bytesPerSample</pt>
                <pd>The number of bytes per audio sample. For example, each PCM audio sample usually takes up 16 bits (2 bytes).</pd>
            </plentry>
            <plentry id="channels">
                <pt>channels</pt>
                <pd>
                    <p>The number of channels.<ul>
                    <li>1: Mono.</li>
                    <li>2: Stereo. If the channel uses stereo, the data is interleaved.</li>
                    </ul></p>
                </pd>
            </plentry>
            <plentry id="samplesPerSec">
                <pt>samplesPerSec</pt>
                <pd>Recording sample rate (Hz).</pd>
            </plentry>
            <plentry id="buffer">
                <pt>buffer</pt>
                <pd>The audio buffer. The buffer size = samplesPerChannel x channels x bytesPerSample.</pd>
            </plentry>
            <plentry id="renderTimeMs">
                <pt>renderTimeMs</pt>
                <pd>The timestamp (ms) of the external audio frame. You can use this parameter for the following purpose: Synchronize audio and video frames in video or audio related scenarios, including where external video sources are used.</pd>
            </plentry>
            <plentry id="avsync" props="android hmos">
                <pt>avsync_type</pt>
                <pd>Reserved for future use.</pd>
            </plentry>
            </parml> </section>
        <section id="return_values" props="native unreal bp unity cs">
            <title>Returns</title>
            <p>Without practical meaning.</p>
        </section>
    </refbody>
</reference>
