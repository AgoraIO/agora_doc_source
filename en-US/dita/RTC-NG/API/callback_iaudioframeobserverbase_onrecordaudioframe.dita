<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="callback_iaudioframeobserverbase_onrecordaudioframe">
    <title><ph keyref="onRecordAudioFrame"/></title>
    <shortdesc id="short"><ph id="shortdesc">Gets the captured audio frame.</ph></shortdesc>
    <prolog>
        <metadata>
            <keywords>
                <indexterm keyref="onRecordAudioFrame"/>
            </keywords>
        </metadata>
    </prolog>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public abstract boolean onRecordAudioFrame(String channelId, int type, int samplesPerChannel,
      int bytesPerSample, int channels, int samplesPerSec, ByteBuffer buffer, long renderTimeMs,
      int avsync_type);</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">- (BOOL)onRecordAudioFrame:(AgoraAudioFrame* _Nonnull)frame channelId:(NSString * _Nonnull)channelId  NS_SWIFT_NAME(onRecordAudioFrame(_:channelId:));</codeblock>
            <codeblock props="cpp unreal" outputclass="language-cpp">virtual bool onRecordAudioFrame(const char* channelId, AudioFrame&amp; audioFrame) = 0;</codeblock>
         <codeblock props="bp" outputclass="language-cpp">DECLARE_DYNAMIC_MULTICAST_DELEGATE_TwoParams(FOnRecordAudioFrame, const FString, channelId, const FAudioFrame&amp;, audioFrame);</codeblock>
            <codeblock props="electron" outputclass="language-typescript">onRecordAudioFrame?(channelId: string, audioFrame: AudioFrame): void;</codeblock>
            <codeblock props="unity cs" outputclass="language-csharp">public virtual bool OnRecordAudioFrame(string channelId, AudioFrame audioFrame)
        {
            return true;
        }</codeblock>
            <codeblock props="rn" outputclass="language-typescript">onRecordAudioFrame?(channelId: string, audioFrame: AudioFrame): void;</codeblock>
            <codeblock props="flutter" outputclass="language-dart">final void Function(String channelId, AudioFrame audioFrame)?
      onRecordAudioFrame;</codeblock>
            <codeblock props="reserve" outputclass="language-cpp"/></p>
        </section>
        <section id="detailed_desc">
            <p props="native unreal bp cs">To ensure that the format of the cpatured audio frame is as expected, you can choose one of the following two methods to set the audio data format:<ul>
            <li>Method 1: After calling <xref keyref="setRecordingAudioFrameParameters"/> to set the audio data format and <xref keyref="registerAudioFrameObserver"/> to register the audio frame observer object, the SDK calculates the sampling interval according to the parameters set in the methods, and triggers the <apiname keyref="onRecordAudioFrame"/> callback according to the sampling interval.</li>
            <li>Method 2: After calling <xref keyref="registerAudioFrameObserver"/> to register the audio frame observer object, set the audio data format in the return value of the <xref keyref="getObservedAudioFramePosition"/> callback. The SDK then calculates the sampling interval according to the return value of the <xref keyref="getRecordAudioParams"/> callback, and triggers the <apiname keyref="onRecordAudioFrame"/> callback according to the sampling interval.</li>
            </ul> </p>
            <p props="flutter electron rn">To ensure that the data format of captured audio frame is as expected, Agora recommends that you set the audio data format as follows: After calling <xref keyref="setRecordingAudioFrameParameters"/> to set the audio data format, call <xref keyref="registerAudioFrameObserver"/> to register the audio observer object, the SDK will calculate the sampling interval according to the parameters set in this method, and triggers the <apiname keyref="onRecordAudioFrame"/> callback according to the sampling interval.</p>
            <note type="note" id="attention" props="native unreal bp flutter cs">
            <ul>
            <li props="native unreal bp cs">The priority of method 1 is higher than that of method 2. If method 1 is used to set the audio data format, the setting of method 2 is invalid.</li>
            <li props="flutter">Due to the limitations of Flutter, this callback does not support sending processed audio data back to the SDK.</li>
            </ul></note>
        <p props="unity">To ensure that the data format of captured audio frame is as expected, Agora recommends that you set the audio data format as follows: After calling <xref keyref="setRecordingAudioFrameParameters"/> to set the audio data format, call <xref keyref="registerAudioFrameObserver"/> to register the audio observer object, the SDK will calculate the sampling interval according to the parameters set in this method, and triggers the <apiname keyref="onRecordAudioFrame"/> callback according to the sampling interval.</p></section>
        <section id="parameters">
            <title>Parameters</title>
            <parml props="apple cpp framework">
            <plentry>
                <pt props="cpp framework">audioFrame</pt>
                <pt props="apple">frame</pt>
                <pd>The raw audio data. See <xref keyref="AudioFrame"/>.</pd>
            </plentry>
            <plentry id="id">
                <pt>channelId</pt>
                <pd>The channel ID.</pd>
            </plentry>
            </parml>
            <parml props="android">
            <plentry id="id1">
                <pt>channelId</pt>
                <pd>The channel ID.</pd>
            </plentry>
            <plentry id="type">
                <pt>type</pt>
                <pd>The audio frame type.</pd>
            </plentry>
            <plentry id="samplesPerChannel">
                <pt>samplesPerChannel</pt>
                <pd>The number of samples per channel in the audio frame.</pd>
            </plentry>
            <plentry id="bytesPerSample">
                <pt>bytesPerSample</pt>
                <pd>The number of bytes per audio sample. For example, each PCM audio sample usually takes up 16 bits (2 bytes).</pd>
            </plentry>
            <plentry id="channels">
                <pt>channels</pt>
                <pd>
                    <p>The number of channels.<ul>
                    <li>1: Mono.</li>
                    <li>2: Stereo. If the channel uses stereo, the data is interleaved.</li>
                    </ul></p>
                </pd>
            </plentry>
            <plentry id="samplesPerSec">
                <pt>samplesPerSec</pt>
                <pd>Recording sample rate (Hz).</pd>
            </plentry>
            <plentry id="buffer">
                <pt>buffer</pt>
                <pd>The audio buffer. The buffer size = samplesPerChannel x channels x bytesPerSample.</pd>
            </plentry>
            <plentry id="renderTimeMs">
                <pt>renderTimeMs</pt>
                <pd>The timestamp (ms) of the external audio frame. You can use this parameter for the following purpose: Synchronize audio and video frames in video or audio related scenarios, including where external video sources are used.</pd>
            </plentry>
            <plentry id="avsync">
                <pt>avsync_type</pt>
                <pd>Reserved for future use.</pd>
            </plentry>
            </parml> </section>
        <section id="return_values" props="native unreal bp unity cs">
            <title>Returns</title>
            <p>Without practical meaning.</p>
        </section>
    </refbody>
</reference>
