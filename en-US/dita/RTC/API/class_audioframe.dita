<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="class_audioframe">
    <title><ph keyref="AudioFrame" /></title>
    <shortdesc id="short"><ph id="shortdesc"> Definition of <apiname keyref="AudioFrame" />.</ph></shortdesc>
    <refbody>
        <section id="prototype">
            <p props="rtc-ng" outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public class AudioFrame {
    public byte[] bytes;
    public int sampleRataHz;
    public int bytesPerSample;
    public int channelNums;
    public int samplesPerChannel;
    public long timestamp;
    @CalledByNative
    public AudioFrame(byte[] bytes, int sampleRataHz, int bytesPerSample, int channelNums,
        int samplesPerChannel, long timestamp) {
      this.sampleRataHz = sampleRataHz;
      this.bytesPerSample = bytesPerSample;
      this.channelNums = channelNums;
      this.samplesPerChannel = samplesPerChannel;
      this.timestamp = timestamp;
      this.bytes = bytes;
    }
    @CalledByNative
    public byte[] getBytes() {
      return bytes;
    }
    @CalledByNative
    public int getBytesPerSample() {
      return bytesPerSample;
    }
    @CalledByNative
    public int getChannelNums() {
      return channelNums;
    }
    @CalledByNative
    public int getSampleRataHz() {
      return sampleRataHz;
    }
    @CalledByNative
    public int getSamplesPerChannel() {
      return samplesPerChannel;
    }
    @CalledByNative
    public long getTimestamp() {
      return timestamp;
    }
    @Override
    public String toString() {
      return "AudioFrame{sampleRataHz=" + sampleRataHz + ", bytesPerSample=" + bytesPerSample
          + ", channelNums=" + channelNums + ", samplesPerChannel=" + samplesPerChannel
          + ", timestamp=" + timestamp + '}';
    }
  }
  </codeblock>
            <codeblock props="cpp" outputclass="language-cpp">struct AudioFrame {
    AUDIO_FRAME_TYPE type;
    int samplesPerChannel;
    agora::rtc::BYTES_PER_SAMPLE bytesPerSample;
    int channels;
    int samplesPerSec;
    void* buffer;
    int64_t renderTimeMs;
    int avsync_type;
    AudioFrame() : type(FRAME_TYPE_PCM16),
                   samplesPerChannel(0),
                   bytesPerSample(rtc::TWO_BYTES_PER_SAMPLE),
                   channels(0),
                   samplesPerSec(0),
                   buffer(NULL),
                   renderTimeMs(0),
                   avsync_type(0) {}
  };
</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">__attribute__((visibility("default"))) @interface AgoraAudioFrame: NSObject
 @property (assign, nonatomic) NSInteger samplesPerChannel;
 @property (assign, nonatomic) NSInteger bytesPerSample;
 @property (assign, nonatomic) NSInteger channels;
 @property (assign, nonatomic) NSInteger samplesPerSec;
 @property (strong, nonatomic) NSData* _Nullable buffer;
 @property (assign, nonatomic) int64_t renderTimeMs;
 @property (assign, nonatomic) NSInteger avSyncType;
@end</codeblock>
            <codeblock props="cs" outputclass="language-csharp">public AudioFrame(AUDIO_FRAME_TYPE type, int samplesPerChannel, int bytesPerSample, int channels, int samplesPerSec,
            byte[] buffer, long renderTimeMs, int avsync_type)
        {
            this.type = type;
            this.samplesPerChannel = samplesPerChannel;
            this.bytesPerSample = bytesPerSample;
            this.channels = channels;
            this.samplesPerSec = samplesPerSec;
            this.buffer = buffer;
            this.renderTimeMs = renderTimeMs;
            this.avsync_type = avsync_type;
        }
        public AUDIO_FRAME_TYPE type { set; get; }
        public int samplesPerChannel { set; get; } 
        public int bytesPerSample { set; get; } 
        public int channels { set; get; } 
        public int samplesPerSec { set; get; } 
        public byte[] buffer { set; get; }
        public long renderTimeMs { set; get; }
        public int avsync_type { set; get; }
    }</codeblock></p>
            <p props="rtc" outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public class AudioFrame {
    public ByteBuffer samples;
	public int numOfSamples;
	public int bytesPerSample;
	public int channels;
	public int samplesPerSec;
    public AudioFrame(ByteBuffer samples, int numOfSamples, int bytesPerSample, int channels, int samplesPerSec) {
        this.samples = samples;
        this.numOfSamples = numOfSamples;
        this.bytesPerSample = bytesPerSample;
        this.channels = channels;
        this.samplesPerSec = samplesPerSec;
    }
    @Override
    public String toString() {
        return "AgoraAudioFrame{" +
                "samples=" + samples +
                ", numOfSamples=" + numOfSamples +
                ", bytesPerSample=" + bytesPerSample +
                ", channels=" + channels +
                ", samplesPerSec=" + samplesPerSec +
                '}';
    }
}</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">__attribute__((visibility("default"))) @interface AgoraAudioFrame : NSObject
@property(assign, nonatomic) NSInteger samplesPerChannel NS_SWIFT_NAME(samplesPerChannel);
@property(assign, nonatomic) NSInteger bytesPerSample NS_SWIFT_NAME(bytesPerSample);
@property(assign, nonatomic) NSInteger channels NS_SWIFT_NAME(channels);
@property(assign, nonatomic) NSInteger samplesPerSec NS_SWIFT_NAME(samplesPerSec);
@property(assign, nonatomic) void* _Nullable buffer NS_SWIFT_NAME(buffer);
@property(assign, nonatomic) int64_t renderTimeMs NS_SWIFT_NAME(renderTimeMs);
@property(assign, nonatomic) NSInteger avSyncType NS_SWIFT_NAME(avSyncType);
</codeblock>
            <codeblock props="cpp" outputclass="language-cpp">struct AudioFrame {
    AUDIO_FRAME_TYPE type;
    int samples;
    int bytesPerSample;
    int channels;
    int samplesPerSec;
    void* buffer;
    int64_t renderTimeMs;
    int avsync_type;
    };</codeblock>
            <codeblock props="cs" outputclass="language-csharp">public class AudioFrame
    {
        public AudioFrame()
        {
        }

        public AudioFrame(AUDIO_FRAME_TYPE type, int samples, int bytesPerSample, int channels, int samplesPerSec,
            byte[] buffer, long renderTimeMs, int avsync_type)
        {
            this.type = type;
            this.samples = samples;
            this.bytesPerSample = bytesPerSample;
            this.channels = channels;
            this.samplesPerSec = samplesPerSec;
            this.buffer = buffer;
            this.renderTimeMs = renderTimeMs;
            this.avsync_type = avsync_type;
        }

        public AUDIO_FRAME_TYPE type { set; get; }
        public int samples { set; get; }
        public int bytesPerSample { set; get; }
        public int channels { set; get; } 
        public int samplesPerSec { set; get; } //sampling rate
        public byte[] buffer { set; get; } 
        public IntPtr bufferPtr { set; get; }
        public long renderTimeMs { set; get; }
        public int avsync_type { set; get; }
    }</codeblock>
            <codeblock props="electron" outputclass="language-typescript" />
            <codeblock props="rn" outputclass="language-typescript" />
            <codeblock props="flutter" outputclass="language-dart" /> </p>
        </section>
        <section id="parameters">
            <title><text conref="../conref/conref_api_metadata.dita#conref_api_metadata/property" /> </title>
            <parml>
            <plentry props="cpp cs">
                <pt>type</pt>
                <pd>
                    <p>The type of the audio frame. See <xref keyref="AUDIO_FRAME_TYPE" />.</p>
                </pd>
            </plentry>
            <plentry>
                <pt props="rtc"><ph props="android">numOfSamples</ph><ph props="cpp cs">samples</ph><ph props="ios mac">samplesPerChannel</ph></pt>
                <pt props="rtc-ng">samplesPerChannel</pt>
                <pd>The number of samples per channel in the audio frame.</pd>
            </plentry>
            <plentry>
                <pt>bytesPerSample</pt>
                <pd>The number of bytes per audio sample, which is usually 16-bit (2 bytes).</pd>
            </plentry>
            <plentry>
                <pt props="rtc">channels</pt>
                <pt props="rtc-ng"><ph props="cpp ios mac">channels</ph><ph props="android">channelNums</ph></pt>
                <pd>
                    <p>The number of audio channels (the data are interleaved if stereo).
                    
                    <ul id="ul_zxz_2wt_r4b">
                    <li>1: Mono.</li>
                    <li>2: Stereo.</li>
                    </ul></p>
                </pd>
            </plentry>
            <plentry>
                <pt props="rtc">samplesPerSec</pt>
                <pt props="rtc-ng"><ph props="cpp ios mac">samplesPerSec</ph><ph props="android">sampleRataHz</ph></pt>
                <pd>The number of samples per channel in the audio frame.</pd>
            </plentry>
            <plentry>
                <pt props="rtc-ng"><ph props="cpp ios mac">buffer</ph><ph props="android">bytes</ph></pt>
                <pt props="rtc"><ph props="cpp cs">buffer</ph><ph props="android">samples</ph></pt>
                <pd>
                    <p>The data buffer of the audio frame. When the audio frame uses a stereo channel, the data buffer is interleaved.</p>
                    <p>Buffer data size: <codeph>buffer</codeph> = <codeph>samples</codeph> ×
<codeph>     channels</codeph> × <codeph>bytesPerSample</codeph>.</p>
                </pd>
            </plentry>
            <plentry props="cs">
                <pt>bufferPtr</pt>
                <pd>Pointer to the data buffer.</pd>
            </plentry>
            <plentry props="cpp cs ios mac">
                <pt>renderTimeMs</pt>
                <pd>
                    <p>The timestamp (ms) of the external audio frame.</p>
                    <p>You can use this timestamp to restore the order of the captured audio frame, and synchronize audio and video frames in video scenarios, including scenarios where external video sources are used.</p>
                </pd>
            </plentry>
            <plentry props="ios mac cpp cs">
                <pt><ph props="ios mac">avsyncType</ph><ph props="cpp cs">avsync_type</ph></pt>
                <pd>A reserved parameter.</pd>
            </plentry>
            <plentry props="rtc-ng">
                <pt props="android">timestamp</pt>
                <pd props="android">Timestamp of the audio frame</pd>
            </plentry>
            </parml> </section>
    </refbody>
</reference>