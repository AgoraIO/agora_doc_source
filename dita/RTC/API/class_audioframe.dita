<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="class_audioframe">
    <title><ph keyref="AudioFrame"/></title>
    <shortdesc id="short"><ph id="shortdesc">AudioFrame 定义。</ph></shortdesc>
    <refbody>
        <section id="prototype">
            <p props="rtc-ng" outputclass="codeblock">
                <codeblock props="android" outputclass="language-java">public class AudioFrame {
  public byte[] bytes; 
  public int sampleRataHz;
  public int bytesPerSample;
  public int channelNums;
  public int samplesPerChannel;
  public long timestamp;
  }</codeblock>
                <codeblock props="ios mac" outputclass="language-objectivec">__attribute__((visibility("default"))) @interface AgoraAudioFrame: NSObject
 @property (assign, nonatomic) NSInteger samplesPerChannel;
 @property (assign, nonatomic) NSInteger bytesPerSample;
 @property (assign, nonatomic) NSInteger channels;
 @property (assign, nonatomic) NSInteger samplesPerSec;
 @property (strong, nonatomic) NSData* _Nullable buffer;
 @property (assign, nonatomic) int64_t renderTimeMs;
 @property (assign, nonatomic) NSInteger avSyncType;
@end</codeblock>
            </p>
            <p props="rtc" outputclass="codeblock">
                <codeblock props="android" outputclass="language-java"/>
                <codeblock props="ios mac" outputclass="language-objectivec"/>
                <codeblock props="windows" outputclass="language-cpp">struct AudioFrame {
    AUDIO_FRAME_TYPE type;
    int samples;
    int bytesPerSample;
    int channels;
    int samplesPerSec;
    void* buffer;
    int64_t renderTimeMs;
    int avsync_type;
    };</codeblock>
                <codeblock props="electron" outputclass="language-typescript"/>
                <codeblock props="unity" outputclass="language-csharp"/>
                <codeblock props="rn" outputclass="language-typescript"/>
                <codeblock props="flutter" outputclass="language-dart"/>
            </p>
        </section>
        <section id="parameters">
   <title><text conref="../conref/conref_api_metadata.dita#metadata/property"/></title>
   <parml>
       <plentry>
  <pt>type</pt>
  <pd>
      <p>音频帧类型，详见 <xref keyref="AUDIO_FRAME_TYPE"/>。</p>
      </pd>
       </plentry>
       <plentry>
  <pt>samples</pt>
  <pd>每个声道的采样点数。</pd>
       </plentry>
       <plentry>
  <pt>bytesPerSample</pt>
  <pd>每个采样点的字节数: 对于 PCM 来说，一般使用 16 bit，即两个字节。</pd>
       </plentry>
       <plentry>
  <pt>channels</pt>
  <pd>
      <p>声道数量(如果是立体声，数据是交叉的)。<ul id="ul_zxz_2wt_r4b">
     <li>1: 单声道</li>
     <li>2: 双声道</li>
 </ul></p>
  </pd>
       </plentry>
       <plentry>
  <pt>samplesPerSec</pt>
  <pd>每声道每秒的采样点数。</pd>
       </plentry>
       <plentry>
  <pt>buffer</pt>
  <pd>
      <p>声音数据缓存区（如果是立体声，数据是交叉存储的）。</p>
      <p>缓存区数据大小 <codeph>buffer</codeph> = <codeph>samples</codeph> ×
     <codeph>channels</codeph> × <codeph>bytesPerSample</codeph>。</p>
  </pd>
       </plentry>
       <plentry>
  <pt>renderTimeMs</pt>
  <pd>
      <p>外部音频帧的渲染时间戳。</p>
      <p>你可以使用该时间戳还原音频帧顺序；在有视频的场景中（包含使用外部视频源的场景），该参数可以用于实现音视频同步。</p>
  </pd>
       </plentry>
       <plentry>
  <pt>avsync_type</pt>
  <pd>保留参数。</pd>
       </plentry>
   </parml>
        </section></refbody>
</reference>
