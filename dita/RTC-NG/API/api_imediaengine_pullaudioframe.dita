<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="api_imediaengine_pullaudioframe">
    <title><ph keyref="pullAudioFrame"/></title>
    <shortdesc id="short"><ph id="shortdesc">拉取远端音频数据。</ph></shortdesc>
    <prolog>
        <metadata>
            <keywords>
                <indexterm keyref="pullAudioFrame"/>
            </keywords>
        </metadata>
    </prolog>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public abstract int pullPlaybackAudioFrame(byte[] data, int lengthInByte);</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">- (BOOL)pullPlaybackAudioFrameRawData:(void * _Nonnull)data
                              lengthInByte:(NSUInteger)lengthInByte;</codeblock>
            <codeblock props="cpp" outputclass="language-cpp">virtual int pullAudioFrame(IAudioFrameObserver::AudioFrame* frame) = 0;</codeblock>
            <codeblock props="electron" outputclass="language-typescript"/>
            <codeblock props="unity" outputclass="language-csharp">public abstract int PullAudioFrame(AudioFrame frame);</codeblock>
            <codeblock props="rn" outputclass="language-typescript"/>
            <codeblock props="flutter" outputclass="language-dart"/> </p>
        </section>
        <section id="detailed_desc">
            <p props="unity">使用该方法前，你需要在 <xref keyref="RtcEngineConfig"/> 中设置 <parmname props="unity">enableAudioDevice</parmname> 为 <codeph><ph keyref="false"/></codeph>， 并调用 <xref keyref="setExternalAudioSink"/> 通知 app 开启并设置外部渲染。</p>
            <p>调用该方法后，app 会采取主动拉取的方式获取远端已解码和混音后的音频数据，用于音频播放。</p>
            <note type="attention">
            <ul>
            <li props="unity">该方法仅支持拉取自采集的数据。如果你需要拉取 SDK 采集的数据，请不要调用该方法。</li>
            <li>该方法需要在加入频道后调用。</li>
            <li props="unity">开启外部音频渲染后，app 将无法从 <xref keyref="onPlaybackAudioFrame"/> 回调中获得数据。</li>
            <li>该方法和 <apiname keyref="onPlaybackAudioFrame"/> 回调相比，区别在于：
               
                <ul>
                <li>SDK 通过 <apiname keyref="onPlaybackAudioFrame"/> 回调将音频数据传输给 app。如果 app 处理延时，可能会导致音频播放抖动。</li>
                <li>调用该方法后 app 会主动拉取音频数据。通过设置音频数据，SDK 可以调整缓存，帮助 app 处理延时，从而有效避免音频播放抖动。</li>
                </ul></li>
            </ul> </note> </section>
        <section id="parameters">
            <title>参数</title>
            <parml>
            <plentry props="cpp unity">
                <pt>frame</pt>
                <pd>指向 <xref keyref="AudioFrame"/> 的指针。</pd>
            </plentry>
            <plentry props="android ios mac">
                <pt>data</pt>
                <pd>待拉取的远端音频数据，数据类型为 <codeph>byte[]</codeph>。</pd>
            </plentry>
            <plentry props="android ios mac" id="length">
                <pt>lengthInByte</pt>
                <pd>音频数据长度，单位为字节。该参数的值由音频数据时长、<apiname keyref="setExternalAudioSink"/> 的 <codeph>sampleRate</codeph> 和 <codeph>channels</codeph> 参数确定。<codeph>lengthInByte</codeph> = <codeph>sampleRate</codeph>/1000 × 2 × <codeph>channels</codeph> × 音频数据时长 (ms)。</pd>
            </plentry>
            </parml> </section>
        <section id="return_values">
            <title>返回值</title>
            <ul props="android cpp unity">
            <li>0：方法调用成功。</li>
            <li>&lt; 0：方法调用失败。</li>
            </ul>
            <ul props="ios mac">
            <li><codeph><ph keyref="true"/></codeph>：方法调用成功。</li>
            <li><codeph><ph keyref="false"/></codeph>：方法调用失败。</li>
            </ul> </section>
    </refbody>
</reference>
