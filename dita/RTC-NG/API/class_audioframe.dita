<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="class_audioframe">
    <title><ph keyref="AudioFrame" /></title>
    <shortdesc id="short"><ph id="shortdesc">原始音频数据。</ph></shortdesc>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public class AudioFrame {
    public byte[] bytes;
    public int sampleRataHz;
    public int bytesPerSample;
    public int channelNums;
    public int samplesPerChannel;
    public long timestamp;
    @CalledByNative
    public AudioFrame(byte[] bytes, int sampleRataHz, int bytesPerSample, int channelNums,
        int samplesPerChannel, long timestamp) {
      this.sampleRataHz = sampleRataHz;
      this.bytesPerSample = bytesPerSample;
      this.channelNums = channelNums;
      this.samplesPerChannel = samplesPerChannel;
      this.timestamp = timestamp;
      this.bytes = bytes;
    }
    @CalledByNative
    public byte[] getBytes() {
      return bytes;
    }
    @CalledByNative
    public int getBytesPerSample() {
      return bytesPerSample;
    }
    @CalledByNative
    public int getChannelNums() {
      return channelNums;
    }
    @CalledByNative
    public int getSampleRataHz() {
      return sampleRataHz;
    }
    @CalledByNative
    public int getSamplesPerChannel() {
      return samplesPerChannel;
    }
    @CalledByNative
    public long getTimestamp() {
      return timestamp;
    }
    @Override
    public String toString() {
      return "AudioFrame{sampleRataHz=" + sampleRataHz + ", bytesPerSample=" + bytesPerSample
          + ", channelNums=" + channelNums + ", samplesPerChannel=" + samplesPerChannel
          + ", timestamp=" + timestamp + '}';
    }
  }
</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">__attribute__((visibility("default"))) @interface AgoraAudioFrame: NSObject
 @property (assign, nonatomic) NSInteger samplesPerChannel;
 @property (assign, nonatomic) NSInteger bytesPerSample;
 @property (assign, nonatomic) NSInteger channels;
 @property (assign, nonatomic) NSInteger samplesPerSec;
 @property (strong, nonatomic) NSData* _Nullable buffer;
 @property (assign, nonatomic) int64_t renderTimeMs;
 @property (assign, nonatomic) NSInteger avSyncType;
@end</codeblock>
            <codeblock props="cpp" outputclass="language-cpp">struct AudioFrame {
    AUDIO_FRAME_TYPE type;
    int samplesPerChannel;
    agora::rtc::BYTES_PER_SAMPLE bytesPerSample;
    int channels;
    int samplesPerSec;
    void* buffer;
    int64_t renderTimeMs;
    int avsync_type;
    AudioFrame() : type(FRAME_TYPE_PCM16),
                   samplesPerChannel(0),
                   bytesPerSample(rtc::TWO_BYTES_PER_SAMPLE),
                   channels(0),
                   samplesPerSec(0),
                   buffer(NULL),
                   renderTimeMs(0),
                   avsync_type(0) {}
  };
</codeblock>
            <codeblock props="electron" outputclass="language-typescript">export class AudioFrame {
  
  type?: AudioFrameType;
  
  samplesPerChannel?: number;
  
  bytesPerSample?: BytesPerSample;
  
  channels?: number;
  
  samplesPerSec?: number;
  
  buffer?: Uint8Array;
  
  renderTimeMs?: number;
  
  avsync_type?: number;
}</codeblock>
            <codeblock props="unity" outputclass="language-csharp">public class AudioFrame
    {
        public AudioFrame()
        {
            type = AUDIO_FRAME_TYPE.FRAME_TYPE_PCM16;
            samplesPerChannel = 0;
            bytesPerSample = BYTES_PER_SAMPLE.TWO_BYTES_PER_SAMPLE;
            channels = 0;
            samplesPerSec = 0;
            RawBuffer = new byte[0];
            renderTimeMs = 0;
            avsync_type = 0;
        }

        public AudioFrame(AUDIO_FRAME_TYPE type, int samplesPerChannel, BYTES_PER_SAMPLE bytesPerSample, int channels, int samplesPerSec,
            byte[] buffer, long renderTimeMs, int avsync_type)
        {
            this.type = type;
            this.samplesPerChannel = samplesPerChannel;
            this.bytesPerSample = bytesPerSample;
            this.channels = channels;
            this.samplesPerSec = samplesPerSec;
            this.RawBuffer = buffer;
            this.renderTimeMs = renderTimeMs;
            this.avsync_type = avsync_type;
        }

        public AUDIO_FRAME_TYPE type { set; get; }
        public int samplesPerChannel { set; get; }
        public BYTES_PER_SAMPLE bytesPerSample { set; get; }
        public int channels { set; get; }
        public int samplesPerSec { set; get; }
        public byte[] RawBuffer { set; get; }
        public long renderTimeMs { set; get; }
        public int avsync_type { set; get; }
    }</codeblock>
            <codeblock props="rn" outputclass="language-typescript">export class AudioFrame {
  
  type?: AudioFrameType;
  
  samplesPerChannel?: number;
  
  bytesPerSample?: BytesPerSample;
  
  channels?: number;
  
  samplesPerSec?: number;
  
  buffer?: Uint8Array;
  
  renderTimeMs?: number;
  
  avsync_type?: number;
}</codeblock>
            <codeblock props="flutter" outputclass="language-dart">There are no corresponding names available</codeblock> </p>
        </section>
        <section id="parameters">
            <title> <text conref="../conref/conref_api_metadata.dita#conref_api_metadata/property" /> </title>
            <parml>
            <plentry props="cpp unity">
                <pt>type</pt>
                <pd>
                    <p>音频帧类型，详见 <xref keyref="AUDIO_FRAME_TYPE" />。</p>
                </pd>
            </plentry>
            <plentry>
                <pt>samplesPerChannel</pt>
                <pd>每个声道的采样点数。</pd>
            </plentry>
            <plentry>
                <pt>bytesPerSample</pt>
                <pd>每个采样点的字节数: 对于 PCM 来说，一般使用 16 bit，即两个字节。</pd>
            </plentry>
            <plentry>
                <pt><ph props="cpp ios mac unity">channels</ph><ph props="android">channelNums</ph></pt>
                <pd>
                    <p>声道数量(如果是立体声，数据是交叉的)。
                        <ul id="ul_zxz_2wt_r4b">
                    <li>1: 单声道</li>
                    <li>2: 双声道</li>
                    </ul> </p>
                </pd>
            </plentry>
            <plentry>
                <pt props="cpp ios mac unity">samplesPerSec</pt>
                <pt props="android">sampleRataHz</pt>
                <pd>每声道每秒的采样点数。</pd>
            </plentry>
            <plentry>
                <pt props="cpp ios mac">buffer</pt>
                    <pt props="android">bytes</pt>
                <pt props="unity">RawBuffer</pt>
                <pd>
                    <p>声音数据缓存区（如果是立体声，数据是交叉存储的）。</p>
                    <p>缓存区数据大小 <codeph>buffer</codeph> = <codeph>samples</codeph> × <codeph>channels</codeph> × <codeph>bytesPerSample</codeph>。</p>
                </pd>
            </plentry>
            <plentry props="cpp unity ios mac">
                <pt>renderTimeMs</pt>
                <pd>
                    <p>外部音频帧的渲染时间戳。</p>
                    <p>你可以使用该时间戳还原音频帧顺序；在有视频的场景中（包含使用外部视频源的场景），该参数可以用于实现音视频同步。</p>
                </pd>
            </plentry>
            <plentry props="ios mac cpp unity">
                <pt> <ph props="ios mac">avsyncType</ph> <ph props="cpp unity">avsync_type</ph> </pt>
                <pd>保留参数。</pd>
            </plentry>
            <plentry props="android">
                <pt>timestamp</pt>
                <pd>&gt;音频帧的时间戳。</pd>
            </plentry>
            </parml> </section>
    </refbody>
</reference>