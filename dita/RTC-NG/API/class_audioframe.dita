<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//EN" "reference.dtd">
<reference id="class_audioframe">
    <title><ph keyref="AudioFrame" /></title>
    <shortdesc id="short"><ph id="shortdesc">原始音频数据。</ph></shortdesc>
    <refbody>
        <section id="prototype">
            <p outputclass="codeblock">
            <codeblock props="android" outputclass="language-java">public class AudioFrame {
  public ByteBuffer buffer;
  public int sampleRataHz;
  public int bytesPerSample;
  public int channelNums;
  public int samplesPerChannel;
  public long timestamp;

  @CalledByNative
  public AudioFrame(ByteBuffer buffer, int sampleRataHz, int bytesPerSample, int channelNums,
      int samplesPerChannel, long timestamp) {
    this.sampleRataHz = sampleRataHz;
    this.bytesPerSample = bytesPerSample;
    this.channelNums = channelNums;
    this.samplesPerChannel = samplesPerChannel;
    this.timestamp = timestamp;
    this.buffer = buffer;
  }

  @CalledByNative
  public ByteBuffer getByteBuffer() {
    return buffer;
  }

  @CalledByNative
  public int getBytesPerSample() {
    return bytesPerSample;
  }

  @CalledByNative
  public int getChannelNums() {
    return channelNums;
  }

  @CalledByNative
  public int getSampleRataHz() {
    return sampleRataHz;
  }

  @CalledByNative
  public int getSamplesPerChannel() {
    return samplesPerChannel;
  }

  @CalledByNative
  public long getTimestamp() {
    return timestamp;
  }

  @Override
  public String toString() {
    return "AudioFrame{sampleRataHz=" + sampleRataHz + ", bytesPerSample=" + bytesPerSample
        + ", channelNums=" + channelNums + ", samplesPerChannel=" + samplesPerChannel
        + ", timestamp=" + timestamp + '}';
  }
}</codeblock>
            <codeblock props="ios mac" outputclass="language-objectivec">
__attribute__((visibility("default"))) @interface AgoraAudioFrame : NSObject

@property(assign, nonatomic) NSInteger samplesPerChannel;

@property(assign, nonatomic) NSInteger bytesPerSample;

@property(assign, nonatomic) NSInteger channels;

@property(assign, nonatomic) NSInteger samplesPerSec;

@property(assign, nonatomic) void* _Nullable buffer;

@property(assign, nonatomic) int64_t renderTimeMs;

@property(assign, nonatomic) NSInteger avSyncType;
@end</codeblock>
            <codeblock props="cpp unreal" outputclass="language-cpp">  struct AudioFrame {
    AUDIO_FRAME_TYPE type;
    int samplesPerChannel;
    agora::rtc::BYTES_PER_SAMPLE bytesPerSample;
    int channels;
    int samplesPerSec;
    void* buffer;
    int64_t renderTimeMs;
    int avsync_type;

    AudioFrame() : type(FRAME_TYPE_PCM16),
                   samplesPerChannel(0),
                   bytesPerSample(rtc::TWO_BYTES_PER_SAMPLE),
                   channels(0),
                   samplesPerSec(0),
                   buffer(NULL),
                   renderTimeMs(0),
                   avsync_type(0),
  };
</codeblock>
         <codeblock props="bp" outputclass="language-cpp">USTRUCT(BlueprintType)
struct FAudioFrame {

	GENERATED_BODY()
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	EAUDIO_FRAME_TYPE type;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int samplesPerChannel;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	EBYTES_PER_SAMPLE bytesPerSample;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int channels;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int samplesPerSec;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int64 buffer;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int64 renderTimeMs;
	UPROPERTY(VisibleAnywhere, BlueprintReadWrite, Category = &quot;Agora|AudioFrame&quot;)
	int avsync_type;
};</codeblock>
            <codeblock props="electron" outputclass="language-typescript">export class AudioFrame {
  type?: AudioFrameType;
  samplesPerChannel?: number;
  bytesPerSample?: BytesPerSample;
  channels?: number;
  samplesPerSec?: number;
  buffer?: Uint8Array;
  renderTimeMs?: number;
  avsync_type?: number;
}</codeblock>
            <codeblock props="unity cs" outputclass="language-csharp">public class AudioFrame
    {
        public AudioFrame()
        {
            type = AUDIO_FRAME_TYPE.FRAME_TYPE_PCM16;
            samplesPerChannel = 0;
            bytesPerSample = BYTES_PER_SAMPLE.TWO_BYTES_PER_SAMPLE;
            channels = 0;
            samplesPerSec = 0;
            RawBuffer = new byte[0];
            renderTimeMs = 0;
            avsync_type = 0;
        }

        public AudioFrame(AUDIO_FRAME_TYPE type, int samplesPerChannel, BYTES_PER_SAMPLE bytesPerSample, int channels, int samplesPerSec,
            byte[] buffer, long renderTimeMs, int avsync_type)
        {
            this.type = type;
            this.samplesPerChannel = samplesPerChannel;
            this.bytesPerSample = bytesPerSample;
            this.channels = channels;
            this.samplesPerSec = samplesPerSec;
            this.RawBuffer = buffer;
            this.renderTimeMs = renderTimeMs;
            this.avsync_type = avsync_type;
        }

        public AUDIO_FRAME_TYPE type { set; get; }
        public int samplesPerChannel { set; get; }
        public BYTES_PER_SAMPLE bytesPerSample { set; get; }
        public int channels { set; get; }
        public int samplesPerSec { set; get; }
        public byte[] RawBuffer { set; get; }
        public long renderTimeMs { set; get; }
        public int avsync_type { set; get; }
    }</codeblock>
            <codeblock props="rn" outputclass="language-typescript">export class AudioFrame {
  type?: AudioFrameType;
  samplesPerChannel?: number;
  bytesPerSample?: BytesPerSample;
  channels?: number;
  samplesPerSec?: number;
  buffer?: Uint8Array;
  renderTimeMs?: number;
  avsync_type?: number;
}</codeblock>
            <codeblock props="flutter" outputclass="language-dart">class AudioFrame {
  const AudioFrame(
      {this.type,
      this.samplesPerChannel,
      this.bytesPerSample,
      this.channels,
      this.samplesPerSec,
      this.buffer,
      this.renderTimeMs,
      this.avsyncType});</codeblock>
            <codeblock props="reserve" outputclass="language-cpp"></codeblock></p>
        </section>
        <section id="parameters">
            <title> <text conref="../conref/conref_api_metadata.dita#conref_api_metadata/property" /> </title>
            <parml>
            <plentry props="cpp unreal bp unity rn electron flutter cs">
                <pt>type</pt>
                <pd>
                    <p>音频帧类型，详见 <xref keyref="AUDIO_FRAME_TYPE" />。</p>
                </pd>
            </plentry>
            <plentry>
                <pt>samplesPerChannel</pt>
                <pd>每个声道的采样点数。</pd>
            </plentry>
            <plentry>
                <pt>bytesPerSample</pt>
                <pd>每个采样点的字节数。对于 PCM 来说，一般使用 16 bit，即两个字节。</pd>
            </plentry>
            <plentry>
                <pt><ph props="cpp unreal bp ios mac unity electron rn flutter cs">channels</ph><ph props="android">channelNums</ph></pt>
                <pd>
                    <p>声道数量(如果是立体声，数据是交叉的)。
                        <ul id="ul_zxz_2wt_r4b">
                    <li>1: 单声道</li>
                    <li>2: 双声道</li>
                    </ul> </p>
                </pd>
            </plentry>
            <plentry>
                <pt props="cpp unreal bp ios mac unity electron rn flutter cs">samplesPerSec</pt>
                <pt props="android">sampleRataHz</pt>
                <pd>每声道每秒的采样点数。</pd>
            </plentry>
            <plentry>
                <pt props="android cpp unreal bp ios mac electron rn flutter">buffer</pt>
                <pt props="unity cs">RawBuffer</pt>
                <pd>
                    <p>声音数据缓存区（如果是立体声，数据是交叉存储的）。</p>
                    <p>缓存区数据大小 <codeph>buffer</codeph> = <codeph>samples</codeph> × <codeph>channels</codeph> × <codeph>bytesPerSample</codeph>。</p>
                </pd>
            </plentry>
            <plentry props="cpp unreal bp unity ios mac rn electron flutter cs">
                <pt>renderTimeMs</pt>
                <pd>
                    <p>外部音频帧的渲染时间戳。</p>
                    <p>你可以使用该时间戳还原音频帧顺序；在有视频的场景中（包含使用外部视频源的场景），该参数可以用于实现音视频同步。</p>
                </pd>
            </plentry>
            <plentry props="ios mac cpp unreal bp unity flutter electron rn cs">
                <pt> <ph props="ios mac flutter">avsyncType</ph> <ph props="cpp unreal bp unity electron rn cs">avsync_type</ph> </pt>
                <pd>保留参数。</pd>
            </plentry>
            <plentry props="android">
                <pt>timestamp</pt>
                <pd>音频帧的时间戳。</pd>
            </plentry>
            </parml> </section>
    </refbody>
</reference>